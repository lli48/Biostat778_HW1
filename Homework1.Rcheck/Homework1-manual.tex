\nonstopmode{}
\documentclass[a4paper]{book}
\usepackage[times,inconsolata,hyper]{Rd}
\usepackage{makeidx}
\usepackage[utf8,latin1]{inputenc}
% \usepackage{graphicx} % @USE GRAPHICX@
\makeindex{}
\begin{document}
\chapter*{}
\begin{center}
{\textbf{\huge Package `Homework1'}}
\par\bigskip{\large \today}
\end{center}
\begin{description}
\raggedright{}
\item[Type]\AsIs{Package}
\item[Title]\AsIs{What the package does (short line)}
\item[Version]\AsIs{1.0}
\item[Date]\AsIs{2013-11-09}
\item[Author]\AsIs{Who wrote it}
\item[Maintainer]\AsIs{Who to complain to }\email{yourfault@somewhere.net}\AsIs{}
\item[Description]\AsIs{More about what it does (maybe more than one line)}
\item[License]\AsIs{What license is it under?}
\end{description}
\Rdcontents{\R{} topics documented:}
\inputencoding{utf8}
\HeaderA{Homework1-package}{PH140.778 Homework1}{Homework1.Rdash.package}
\aliasA{Homework1}{Homework1-package}{Homework1}
\keyword{package}{Homework1-package}
%
\begin{Description}\relax


This package gives faster algorithms to fit a linear regression model and compute the multivariate normal density.
\end{Description}
%
\begin{Details}\relax

\Tabular{ll}{
Package: & Homework1\\{}
Type: & Package\\{}
Version: & 1.0\\{}
Date: & 2013-11-08\\{}
License: & GPL\\{}
}


This package mainly uses Cholesky decomposition when computing the inverse matrix.  
\end{Details}
%
\begin{Author}\relax

Lu Li
\end{Author}
%
\begin{References}\relax

PH140.778 Advanced Statistical Computing
\end{References}
\inputencoding{utf8}
\HeaderA{dmvnorm}{Computing the Multivariate Normal Density MORE Efficiently}{dmvnorm}
%
\begin{Description}\relax

This function dmvnorm() evaluates the k-dimensional multivariate Normal density with mean mu and covariance S
\end{Description}
%
\begin{Usage}
\begin{verbatim}
dmvnorm(x, mu, S, log = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] 

a n*k matrix of points to be evaluated
\item[\code{mu}] 

a length k vector of means for the k-dimensional Normal
\item[\code{S}] 

a k*k covariance matrix
\item[\code{log}] 

whether the return should be the log density. Default is TRUE
\end{ldescription}
\end{Arguments}
%
\begin{Value}





This funtion returns a vector of length n containing the values of the multivariate Normal density evaluated at the n points. If log = TRUE, it returns the log density at those points. If log = FALSE, it returns the density values.
\end{Value}
%
\begin{Author}\relax

Lu Li
\end{Author}
%
\begin{Examples}
\begin{ExampleCode}


## The function is currently defined as
function (x, mu, S, log = TRUE) 
{
    k = length(mu)
    n = nrow(x)
    Q = tryCatch({
        chol(S)
    }, error = function(li) {
        message("S cannot be a covariance matrix")
    })
    temp1 = x - rep(1, n) %*% t(mu)
    A = forwardsolve(t(Q), t(temp1))
    temp2 = diag(crossprod(A))
    density = (-k/2) * log(2 * pi) - (1/2) * 2 * sum(log(diag(Q))) - 
        (1/2) * temp2
    if (log == FALSE) {
        density = exp(density)
    }
    return(density)
  }
\end{ExampleCode}
\end{Examples}
\inputencoding{utf8}
\HeaderA{fastlm}{Faster Way to Fit Linear Regression Models}{fastlm}
\keyword{Cholesky decomposition}{fastlm}
\keyword{Linear Regression Model}{fastlm}
%
\begin{Description}\relax

Comparing to the lm.fit() function in R, this fastlm() function presents a much faster way to fit a linear regression model. 
\end{Description}
%
\begin{Usage}
\begin{verbatim}
fastlm(X, y, na.rm = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{X}] 

predictor variable, a n*k matrix

\item[\code{y}] 

response variable, a vector of length n

\item[\code{na.rm}] 

argument indicating that whether missing values in X or y should be removed. Default is FALSE 

\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax

fastlm() takes the advantages of Cholesky decomposition to compute the inverse matrices, which makes a huge improvement in the efficiency of fitting linear regression models. 
\end{Details}
%
\begin{Value}





a list of components 
\begin{ldescription}
\item[\code{coefficients}] 
a vector of the regression coefficients estimated using maximum likelihood

\item[\code{vcov}] 
the p*p covariance matrix of the estimated regression coefficients

\end{ldescription}
\end{Value}
%
\begin{Author}\relax

Lu Li
\end{Author}
%
\begin{References}\relax

PH140.778 Advanced Statistical Computing
\end{References}
%
\begin{Examples}
\begin{ExampleCode}

function (X, y, na.rm = FALSE) 
{
    n <- length(y)
    p <- ncol(X)
    
    ##Check if missing values in X and y should be removed
    if (na.rm == TRUE) {
        Z = cbind(X, y)
        X = X[complete.cases(Z), ]
        y = as.matrix(y[complete.cases(Z)])
    }
    A <- crossprod(X)
    C <- crossprod(X, y)
    
    ##Cholesky decomposition
    Q <- chol(A)
    temp1 <- forwardsolve(t(Q), C)
    betahat <- backsolve(Q, temp1)
    cov_beta <- chol2inv(Q) * as.numeric(crossprod(y - X %*% 
        betahat)/(n - p))
    return(list(coeffients = betahat, vcov = cov_beta))
  }
    set.seed(2)
## Generate predictor matrix
    n <- 100
    p <- 5
    X <- cbind(1, matrix(rnorm(n * (p - 1)), n, p - 1))

## Coefficents
    b <- rnorm(p)

## Response
    y <-X%*%b + rnorm(n)

    fit <- fastlm(X, y)
    str(fit)
\end{ExampleCode}
\end{Examples}
\printindex{}
\end{document}
